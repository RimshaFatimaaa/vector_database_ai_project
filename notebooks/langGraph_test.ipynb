{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906ac409-d570-48dd-a30e-262085eadff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (0.119.0)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (0.37.0)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from fastapi) (0.48.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from fastapi) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from fastapi) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from uvicorn) (8.3.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.78 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (0.3.79)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (2.0.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (3.13.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (0.4.34)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-community) (2.3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.78->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.78->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.78->langchain-community) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.78->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rimsha\\anaconda3\\envs\\langgraph_course\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn\n",
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681144ac-e676-4a98-9e06-390a1e39e71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üöÄ Running Step-4: LangGraph Interview Flow\n",
      "INFO:root:üéØ Generating interview question...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:üß© Evaluating candidate's response...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:üéØ Generating interview question...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Asked Question: Can you describe a challenging project you worked on in the past two years and how you approached problem-solving during that project?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:üß© Evaluating candidate's response...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Candidate Answer: I once handled a conflict by organizing a team meeting and discussing our goals openly to find a shared solution.\n",
      "\n",
      "üìä Evaluation: The candidate's answer provides a glimpse into their approach to conflict resolution within a team setting, which is a valuable skill in software engineering. However, the response lacks specific details about the challenging project itself, such as the project's nature, the obstacles faced, and the technical aspects involved. The mention of organizing a team meeting is a good start, but it would be more impactful if the candidate elaborated on the challenges they encountered and how their actions directly contributed to overcoming those challenges. Overall, the answer could be strengthened by providing more context and demonstrating a deeper involvement in the project's technical aspects and outcomes.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 4: LangGraph + LangChain Interview Flow (Final Fixed Version)\n",
    "# --------------------------------------------------------------------\n",
    "import os\n",
    "import logging\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# üß† Initialize LLM\n",
    "# -------------------------------------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, api_key=api_key)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# üß© Define Interview State\n",
    "# -------------------------------------------------------\n",
    "class InterviewState(BaseModel):\n",
    "    round_type: Optional[str] = None\n",
    "    context: Optional[str] = None\n",
    "    question: Optional[str] = None\n",
    "    candidate_answer: Optional[str] = None\n",
    "    evaluation: Optional[str] = None\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# üîß Define Chains (using RunnableSequence)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Question generation\n",
    "question_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate one interview question for a {round_type} round based on this context: {context}\"\n",
    ")\n",
    "question_chain = question_prompt | llm\n",
    "\n",
    "# Answer evaluation\n",
    "evaluation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Evaluate this candidate answer for the question '{question}': {candidate_answer}. \"\n",
    "    \"Give a brief evaluation summary.\"\n",
    ")\n",
    "evaluation_chain = evaluation_prompt | llm\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# üß± Define Node Functions\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def generate_question(state: InterviewState) -> InterviewState:\n",
    "    logging.info(\"üéØ Generating interview question...\")\n",
    "    result = question_chain.invoke({\n",
    "        \"round_type\": state.round_type or \"General\",\n",
    "        \"context\": state.context or \"software engineering interview\"\n",
    "    })\n",
    "\n",
    "    # Extract text safely\n",
    "    question_text = getattr(result, \"content\", str(result)).strip()\n",
    "\n",
    "    # Update state correctly (avoid duplicate keyword)\n",
    "    state_data = state.model_dump()\n",
    "    state_data[\"question\"] = question_text\n",
    "\n",
    "    return InterviewState(**state_data)\n",
    "\n",
    "def evaluate_response(state: InterviewState) -> InterviewState:\n",
    "    logging.info(\"üß© Evaluating candidate's response...\")\n",
    "    result = evaluation_chain.invoke({\n",
    "        \"question\": state.question,\n",
    "        \"candidate_answer\": state.candidate_answer\n",
    "    })\n",
    "\n",
    "    evaluation_text = getattr(result, \"content\", str(result)).strip()\n",
    "    state_data = state.model_dump()\n",
    "    state_data[\"evaluation\"] = evaluation_text\n",
    "\n",
    "    return InterviewState(**state_data)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# üï∏Ô∏è Build LangGraph\n",
    "# -------------------------------------------------------\n",
    "graph = StateGraph(InterviewState)\n",
    "graph.add_node(\"generate_question\", generate_question)\n",
    "graph.add_node(\"evaluate_response\", evaluate_response)\n",
    "\n",
    "graph.set_entry_point(\"generate_question\")\n",
    "graph.add_edge(\"generate_question\", \"evaluate_response\")\n",
    "graph.add_edge(\"evaluate_response\", END)\n",
    "\n",
    "interview_graph = graph.compile()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# üöÄ Run Flow\n",
    "# -------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"üöÄ Running Step-4: LangGraph Interview Flow\")\n",
    "\n",
    "init_state = InterviewState(\n",
    "    round_type=\"HR\",\n",
    "    context=\"Candidate is a software engineer with 2 years of experience.\"\n",
    ")\n",
    "\n",
    "# 1Ô∏è‚É£ Generate Question\n",
    "result_1 = interview_graph.invoke(init_state)\n",
    "state_after_q = InterviewState(**result_1)\n",
    "\n",
    "print(f\"\\nüß† Asked Question: {state_after_q.question}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Add Candidate's Answer\n",
    "state_after_q.candidate_answer = (\n",
    "    \"I once handled a conflict by organizing a team meeting \"\n",
    "    \"and discussing our goals openly to find a shared solution.\"\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Evaluate\n",
    "result_2 = interview_graph.invoke(state_after_q)\n",
    "final_state = InterviewState(**result_2)\n",
    "\n",
    "print(f\"\\nüí¨ Candidate Answer: {final_state.candidate_answer}\")\n",
    "print(f\"\\nüìä Evaluation: {final_state.evaluation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b80b1",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è LangGraph Advanced Examples\n",
    "\n",
    "This notebook demonstrates advanced LangGraph capabilities including:\n",
    "\n",
    "1. **Multi-step Workflows** - Complex interview processes with multiple decision points\n",
    "2. **Conditional Logic** - Dynamic workflow paths based on candidate responses\n",
    "3. **State Persistence** - Memory management across multiple interactions\n",
    "4. **Error Handling** - Robust error handling and fallback mechanisms\n",
    "5. **Custom Node Functions** - Specialized processing for different interview types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Advanced LangGraph Implementation\n",
    "# Import the new LangGraph processor from the project\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "from ai_modules.langgraph_processor import (\n",
    "    LangGraphInterviewProcessor, \n",
    "    create_interview_processor,\n",
    "    InterviewState\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Advanced LangGraph processor imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b8ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Example 1: Multi-Round Interview Workflow\n",
    "print(\"üéØ Example 1: Multi-Round Interview Workflow\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create processor\n",
    "processor = create_interview_processor(use_openai=True)\n",
    "\n",
    "# Define interview scenarios\n",
    "interview_scenarios = [\n",
    "    {\n",
    "        \"round\": \"HR\",\n",
    "        \"context\": \"Senior software engineer with 5 years experience\",\n",
    "        \"answer\": \"I led a team of 4 developers on a critical project. We used agile methodology and daily standups to ensure smooth communication.\"\n",
    "    },\n",
    "    {\n",
    "        \"round\": \"Technical\", \n",
    "        \"context\": \"Full-stack developer position\",\n",
    "        \"answer\": \"For a REST API, I would use Express.js with proper error handling, input validation, and rate limiting. I'd also implement JWT authentication.\"\n",
    "    },\n",
    "    {\n",
    "        \"round\": \"Behavioral\",\n",
    "        \"context\": \"Team lead role\",\n",
    "        \"answer\": \"When facing a tight deadline, I prioritize tasks based on impact and dependencies, communicate with stakeholders about trade-offs, and ensure quality isn't compromised.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run multiple rounds\n",
    "results = []\n",
    "for i, scenario in enumerate(interview_scenarios, 1):\n",
    "    print(f\"\\nüîÑ Round {i}: {scenario['round']} Interview\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    result = processor.run_interview_round(\n",
    "        round_type=scenario[\"round\"],\n",
    "        context=scenario[\"context\"],\n",
    "        candidate_answer=scenario[\"answer\"]\n",
    "    )\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer: {result['candidate_answer']}\")\n",
    "    print(f\"Score: {result['score']}\")\n",
    "    print(f\"Success: {result['success']}\")\n",
    "\n",
    "# Show session summary\n",
    "print(f\"\\nüìä Session Summary:\")\n",
    "summary = processor.get_session_summary()\n",
    "print(f\"Total Interactions: {summary['total_interactions']}\")\n",
    "print(f\"Round Types: {summary['round_types']}\")\n",
    "print(f\"Average Score: {summary['average_score']:.1f}\")\n",
    "print(f\"Model: {summary['model']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Example 2: Custom State Management\n",
    "print(\"\\nüîß Example 2: Custom State Management\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a new processor for this example\n",
    "processor2 = create_interview_processor(use_openai=True)\n",
    "\n",
    "# Create custom initial state\n",
    "custom_state = InterviewState(\n",
    "    round_type=\"Technical\",\n",
    "    context=\"Machine learning engineer with expertise in deep learning\",\n",
    "    metadata={\n",
    "        \"session_id\": \"demo_001\",\n",
    "        \"candidate_level\": \"senior\",\n",
    "        \"specialization\": \"ML/DL\",\n",
    "        \"experience_years\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Initial State:\")\n",
    "print(f\"  Round Type: {custom_state.round_type}\")\n",
    "print(f\"  Context: {custom_state.context}\")\n",
    "print(f\"  Metadata: {custom_state.metadata}\")\n",
    "\n",
    "# Run workflow with custom state\n",
    "result = processor2.run_interview_round(\n",
    "    round_type=custom_state.round_type,\n",
    "    context=custom_state.context,\n",
    "    candidate_answer=\"I would implement a neural network using TensorFlow, with proper data preprocessing, cross-validation, and hyperparameter tuning. I'd also use techniques like dropout and batch normalization to prevent overfitting.\"\n",
    ")\n",
    "\n",
    "print(f\"\\nWorkflow Result:\")\n",
    "print(f\"  Question: {result['question']}\")\n",
    "print(f\"  Score: {result['score']}\")\n",
    "print(f\"  Current Step: {result['current_step']}\")\n",
    "print(f\"  Success: {result['success']}\")\n",
    "\n",
    "# Show conversation history\n",
    "if result.get('conversation_history'):\n",
    "    print(f\"\\nüìù Conversation History:\")\n",
    "    for i, interaction in enumerate(result['conversation_history'], 1):\n",
    "        print(f\"  Interaction {i}:\")\n",
    "        print(f\"    Round: {interaction.get('round_type', 'Unknown')}\")\n",
    "        print(f\"    Score: {interaction.get('score', {}).get('overall', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ°Ô∏è Example 3: Error Handling and Fallback\n",
    "print(\"\\nüõ°Ô∏è Example 3: Error Handling and Fallback\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with fallback mode (no OpenAI)\n",
    "print(\"Testing fallback mode (no OpenAI API key)...\")\n",
    "\n",
    "# Temporarily remove API key for testing\n",
    "original_key = os.environ.get('OPENAI_API_KEY')\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    del os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Create processor in fallback mode\n",
    "fallback_processor = create_interview_processor(use_openai=False)\n",
    "\n",
    "print(f\"Processor created with OpenAI: {fallback_processor.use_openai}\")\n",
    "print(f\"Model: {fallback_processor.model}\")\n",
    "\n",
    "# Test fallback functionality\n",
    "fallback_result = fallback_processor.run_interview_round(\n",
    "    round_type=\"HR\",\n",
    "    context=\"Software engineer\",\n",
    "    candidate_answer=\"I believe in clear communication and regular team meetings to ensure everyone is aligned on project goals.\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFallback Result:\")\n",
    "print(f\"  Question: {fallback_result['question']}\")\n",
    "print(f\"  Answer: {fallback_result['candidate_answer']}\")\n",
    "print(f\"  Evaluation: {fallback_result['evaluation']}\")\n",
    "print(f\"  Score: {fallback_result['score']}\")\n",
    "print(f\"  Success: {fallback_result['success']}\")\n",
    "\n",
    "# Restore API key\n",
    "if original_key:\n",
    "    os.environ['OPENAI_API_KEY'] = original_key\n",
    "\n",
    "print(f\"\\n‚úÖ Error handling test completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e007b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Example 4: Interactive Interview Simulation\n",
    "print(\"\\nüé® Example 4: Interactive Interview Simulation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def interactive_interview():\n",
    "    \"\"\"Interactive interview simulation\"\"\"\n",
    "    processor = create_interview_processor(use_openai=True)\n",
    "    \n",
    "    print(\"üéÆ Interactive Interview Simulation\")\n",
    "    print(\"You can provide your own answers to see how the system evaluates them.\")\n",
    "    print(\"Type 'quit' to exit the simulation.\\n\")\n",
    "    \n",
    "    round_count = 0\n",
    "    \n",
    "    while True:\n",
    "        round_count += 1\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Round {round_count}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        round_type = input(\"Enter round type (HR/Technical/Behavioral) or 'quit': \").strip()\n",
    "        \n",
    "        if round_type.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        if round_type not in ['HR', 'Technical', 'Behavioral']:\n",
    "            print(\"‚ùå Invalid round type. Please use HR, Technical, or Behavioral.\")\n",
    "            continue\n",
    "        \n",
    "        context = input(\"Enter candidate context (or press Enter for default): \").strip()\n",
    "        if not context:\n",
    "            context = f\"Candidate applying for {round_type} position\"\n",
    "        \n",
    "        # Generate question\n",
    "        print(\"\\nü§ñ Generating question...\")\n",
    "        question_result = processor.run_interview_round(\n",
    "            round_type=round_type,\n",
    "            context=context,\n",
    "            candidate_answer=None\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüß† Question: {question_result['question']}\")\n",
    "        \n",
    "        # Get user answer\n",
    "        user_answer = input(\"\\nüí¨ Your answer: \").strip()\n",
    "        \n",
    "        if not user_answer:\n",
    "            print(\"‚ùå No answer provided. Skipping evaluation.\")\n",
    "            continue\n",
    "        \n",
    "        # Evaluate answer\n",
    "        print(\"\\nü§ñ Evaluating your answer...\")\n",
    "        evaluation_result = processor.run_interview_round(\n",
    "            round_type=round_type,\n",
    "            context=context,\n",
    "            candidate_answer=user_answer\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìä Evaluation: {evaluation_result['evaluation']}\")\n",
    "        print(f\"‚≠ê Score: {evaluation_result['score']}\")\n",
    "    \n",
    "    # Show final summary\n",
    "    print(f\"\\nüìà Final Session Summary:\")\n",
    "    summary = processor.get_session_summary()\n",
    "    print(f\"Total Interactions: {summary['total_interactions']}\")\n",
    "    print(f\"Round Types: {summary['round_types']}\")\n",
    "    print(f\"Average Score: {summary['average_score']:.1f}\")\n",
    "    print(f\"Model Used: {summary['model']}\")\n",
    "    \n",
    "    print(\"\\nüëã Thanks for trying the interactive simulation!\")\n",
    "\n",
    "# Uncomment the line below to run interactive simulation\n",
    "# interactive_interview()\n",
    "\n",
    "print(\"üí° To run the interactive simulation, uncomment the last line in this cell!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebbb234",
   "metadata": {},
   "source": [
    "# üìö LangGraph Implementation Summary\n",
    "\n",
    "## ‚úÖ What We've Implemented\n",
    "\n",
    "### 1. **Core LangGraph Module** (`ai_modules/langgraph_processor.py`)\n",
    "- **InterviewState**: Pydantic model for state management\n",
    "- **LangGraphInterviewProcessor**: Main processor class with workflow orchestration\n",
    "- **Memory Management**: Conversation history and state persistence\n",
    "- **Error Handling**: Robust fallback mechanisms\n",
    "\n",
    "### 2. **Integration with Existing App** (`app.py`)\n",
    "- **New Mode**: \"LangGraph Workflow\" option in the Streamlit app\n",
    "- **UI Components**: Dedicated interface for LangGraph functionality\n",
    "- **Display Functions**: Specialized result visualization\n",
    "\n",
    "### 3. **Testing Suite** (`test_langgraph.py`)\n",
    "- **Unit Tests**: Comprehensive test coverage\n",
    "- **Integration Tests**: End-to-end workflow testing\n",
    "- **Error Handling Tests**: Fallback mechanism validation\n",
    "\n",
    "### 4. **Demo Script** (`demo_langgraph.py`)\n",
    "- **Interactive Demo**: Hands-on LangGraph experience\n",
    "- **Multiple Scenarios**: Different interview types and contexts\n",
    "- **Session Management**: Complete workflow demonstration\n",
    "\n",
    "### 5. **Enhanced Notebook** (`notebooks/langGraph_test.ipynb`)\n",
    "- **Advanced Examples**: Multi-step workflows and state management\n",
    "- **Interactive Features**: User-driven interview simulation\n",
    "- **Error Handling**: Fallback mode demonstration\n",
    "\n",
    "## üöÄ Key Features\n",
    "\n",
    "### **Workflow Orchestration**\n",
    "- **Multi-step Process**: Question generation ‚Üí Evaluation ‚Üí History tracking\n",
    "- **State Management**: Persistent state across workflow steps\n",
    "- **Conditional Logic**: Dynamic workflow paths based on responses\n",
    "\n",
    "### **Memory & Persistence**\n",
    "- **Conversation History**: Track all interactions\n",
    "- **Session Summary**: Aggregate statistics and insights\n",
    "- **State Persistence**: Maintain context across sessions\n",
    "\n",
    "### **Error Handling**\n",
    "- **Fallback Mode**: Works without OpenAI API\n",
    "- **Graceful Degradation**: Continues operation on errors\n",
    "- **Robust Recovery**: Automatic error recovery mechanisms\n",
    "\n",
    "### **Integration**\n",
    "- **Streamlit App**: Seamless integration with existing UI\n",
    "- **Backward Compatibility**: Works with existing interview flow\n",
    "- **Modular Design**: Easy to extend and customize\n",
    "\n",
    "## üéØ Usage Examples\n",
    "\n",
    "### **Basic Usage**\n",
    "```python\n",
    "from ai_modules.langgraph_processor import create_interview_processor\n",
    "\n",
    "processor = create_interview_processor(use_openai=True)\n",
    "result = processor.run_interview_round(\n",
    "    round_type=\"HR\",\n",
    "    context=\"Software engineer\",\n",
    "    candidate_answer=\"I believe in teamwork and communication.\"\n",
    ")\n",
    "```\n",
    "\n",
    "### **Advanced Usage**\n",
    "```python\n",
    "from ai_modules.langgraph_processor import InterviewState\n",
    "\n",
    "custom_state = InterviewState(\n",
    "    round_type=\"Technical\",\n",
    "    context=\"Senior developer\",\n",
    "    metadata={\"level\": \"senior\", \"focus\": \"backend\"}\n",
    ")\n",
    "\n",
    "result = processor.run_interview_round(\n",
    "    round_type=custom_state.round_type,\n",
    "    context=custom_state.context,\n",
    "    candidate_answer=\"I would use microservices architecture...\"\n",
    ")\n",
    "```\n",
    "\n",
    "## üîß Configuration\n",
    "\n",
    "### **Environment Variables**\n",
    "- `OPENAI_API_KEY`: Required for full functionality\n",
    "- Falls back to basic mode if not available\n",
    "\n",
    "### **Model Configuration**\n",
    "- **Default Model**: `gpt-4o-mini`\n",
    "- **Temperature**: `0.7` for balanced creativity/consistency\n",
    "- **Customizable**: Easy to modify model parameters\n",
    "\n",
    "## üìä Performance Metrics\n",
    "\n",
    "### **Session Tracking**\n",
    "- Total interactions\n",
    "- Average scores\n",
    "- Round type distribution\n",
    "- Memory status\n",
    "\n",
    "### **Workflow Metrics**\n",
    "- Success rate\n",
    "- Processing time\n",
    "- Error frequency\n",
    "- Fallback usage\n",
    "\n",
    "## üéâ Next Steps\n",
    "\n",
    "1. **Run the Demo**: Execute `python demo_langgraph.py`\n",
    "2. **Test the App**: Use \"LangGraph Workflow\" mode in Streamlit\n",
    "3. **Explore Examples**: Run cells in `langGraph_test.ipynb`\n",
    "4. **Run Tests**: Execute `python test_langgraph.py`\n",
    "\n",
    "The LangGraph implementation is now fully integrated and ready for use! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c722764-530a-4595-be65-69dc291670d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langGraph_course]",
   "language": "python",
   "name": "conda-env-langGraph_course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
